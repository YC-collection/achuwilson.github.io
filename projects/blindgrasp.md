---
title: "BlindGrasp -  Robot grasping using only tactile senses in visually inaccessible environments"
author: achu
layout: post
permalink: /projects/blindgrasp
categories:
  - Research
  - Manipulation
  - Grasping
  
 
     
excerpt: "BlindGrasp -   Robot grasping using only tactile senses in visually inaccessible environments"
---


The sense of touch is one of the most powerful and amazing senses that humans have. It provides rich information about the environment that we make contact with.  It is so powerfull that the visually impaired people mainly explores the world using the sense of touch. Even for the normal people, the sense of touch is deeply intewined with our brain that we often use it unconsiously. For example, we would not look into our pocket before grabbing a coin or key from it. Still, everytime, we can succesfully do it. We can succesfull grasp items from visually inaccessible areas using the sense of touch only

Tactile sensing in robots are not widely used unlike the vision systems. Probably too much dependance on the vision systems may have made us ignore the rich information provided by the tactile sensing.The unavailability of high resoultion tactile sensors may also have been a reason for this. Most of the approaches on using tactile sensing are just confined to the detection of slip or quality of grasp, estimating mechanical properties like hardness of objects etc. 

**Project BlindGrasp aims to explore the usage of tactile sensing for grasping in unstructured and visually inaccessible environments**. I began seriously thinking about it after coming to know about a high resoultion tactile sensor named GelSight. I came to know about Gelsight at ICRA 2017 from a couple of MIT PhD students( Wenzhen Yuan, Greg Izzat & Geronimo Mirano) working on it. They were at ICRA to present their research on using Gelsight to augment the 3D pointcloud generated by the vision systems.


{% include image.html url="https://farm5.staticflickr.com/4482/37886429841_a45491c7a8_k.jpg" caption="My fingerprint on the Gelsight Gripper " href="https://farm5.staticflickr.com/4482/37886429841_a45491c7a8_k.jpg" width=600 align="center" %}

## **Project Goals:**
Blindgrasp needs both research into mechanical structure of the Gelsight gripper and the software algorithms to make sense of the data. They are as follows.

###1. GelSight Finger
  The original GelSight gripper consists of a planar sensing area of size 24x18mm. It works well for tasks like estimating hardness of objects, generating pointclouds for object recognition etc. But it will be inefficient for exploring tasks in clutter, since it can only see forces coming from a single direction. In case of human fingerrs, they are  cylindrical in shape and the most sensitive area is the inner semi cylindrical finger tip. So, one of the hardware enhancements would be to model and design a curved Gelsight sensor and gripper. It is best illustrated in the figure below
  
 <br>
{% include image.html url="/images/gelsight_curved.png" href="../projects/blindgrasp" caption="Tactile exploration in clutter would require a curved GelSight Gripper" width=480  align="center" %}

<br>
  

  In addition to the mechanical design,this would also include non linear remapping of the touch information on the curved surface onto the planar camera sensor 
  <br>
  

###2. Fingerprint for the Gripper
  The original GelSight depends on the features on the object to detect contact. It may fail with featureless smooth surfaces like glass. Adding features like ridges similar to fingerprints on the outer sensing surface of GelSight can help in detecting contact with featureless surfaces. It could also  help in generating vibrations when the finger is moved over textured surfaces. This would be another hardware enhancement
  
  
###3. Tactile exploration using Deep Reinforcement Learning
   This involves developing a tactile exploration policy for the control of a manipulator. The exploration policy would generate control commands (joint torques) for the manipulator based on tactile feedback such that it can search through a cluttered environment and pick up the desired object.I have set a goal of picking up a coin from a tray filled with small spherical marbles. This would be a good scenario to justify the usage of tactile exploration, as it would be impossible to detect the coin with vision if it is buried under the marbles. The initial approach would be to use a deep reinforcement learning based agent to pick up the coin. The agent will have to explore by moving the robot's end effector and digging through the marbles. It will be given positive rewards when it comes in contact with the coin and a final grand reward when it picks up the coin. This would also involve developing of new reinforcement agents which will work well with sparse rewards since the exploration phase would be often having zero rewards. 

   **Yes,there is a hidden coin under the marbles,  the robot has to dig into it, find it and pick it up.**

<br>
{% include image.html url="/images/kuka_env_marbles.png" href="../projects/blindgrasp" caption="Yes,there is a hidden coin under the marbles,  the robot has to dig into it, find it and pick it up." width=480  align="center" %}

<br>


## Update-1 : Nov-22-2017: Simulation of the original Planar Gelsight Sensor

I tried to simulate Gelsight when I was right back from the conference. At first I tried to simulate it in Drake as directed by Greg, but installing Drake and getting it working was a real mess. So, I began trying in Gazebo. But that also did not turn out well since I could not get the contact sensing to work reliably. Then I gave a chance to the physics simulator Bullet. It had softbody simulation, which I thought would be useful for simulating the elastomer of Gelsight. But it turned out that the softbody simulation was a basic and needed much more development. So, I indirectly modelled Gelsight using the raytest functionality in bullet. It returns the depth at which a ray makes contact with a solid body.
    Following is the  video of the simulated gelsight gripper. The Kuka iiwa has a a WSG 50 gripper fitted with a gelsight sensor. The sensor has a sensing area of 24x24mm and has a resolution of 256x256 pixels in the planar sensing area. It outputs standard ROS 3D pointcloud data, which is displayed in RViz.The simulation was run in my laptop and the pointcloud could be generated at 5Hz

 <div align="center">
<iframe width="480" height="270" src="https://www.youtube.com/embed/httoZpXy4nw" frameborder="0" allowfullscreen></iframe>
</div>










<br>
